---
editor_options: 
  markdown: 
    wrap: 72
---

# Lecture 2

### April 6, 2023

## ProPublica Reading

Evaluating and remedying fairness/unfairness requires definition of
fairness

Only 20% of people predicted to commit violent crimes went on to do so
(20% accuracy)

**P( commit crime \| predicted crime ) = 0.20**

-   Algorithm was particularly likely to falsely flag black defendants
    as future criminals, labeling them at twice the rate as white
    defendants (i.e. even less accurate for black defendants than for
    white defendants)
-   White defendants mislabeled as low risk more often than black
    defendants
-   Additional test isolating effect of race, age, and gender from
    criminal history and recidivism, still yielded 77% greater
    likelihood of black defendants being labeled as higher risk

**No P-Values:** skepticism of P-value relevance

After deal reached by prosecution and defense for plea bargain, judge
rejected in favor of heavier sentence on account of algorithm's high
risk prediction score

**Take education beyond 4th grade into account?** \* Argument A: unfair
to hold against individual \* Argument B: lack of education yields
higher risk for unemployment, which yields higher risk for crime \*
Argument C: agree to lighter sentence contingent upon participation in
literacy program

**Taking out sensitive variables:** Removing ***race*** variable for
example, does not eliminate the effects of race. Other highly correlated
variables, such as ***education***, may serve as a ***proxy*** for race
\* Education \* Past criminal record \* Poverty \* Employment \* Social
marginalization

**Risk score histograms:** show that algorithm generally assigns lower
risk to white defendants, but we still need to compare pairs of
comparable individuals of each category \* Consider population/sample
size

**Ceteris paribus:** "all else equal" Example: Find two defendants with
different race but same age/education/history/crime severity/etc, and
compare scores

**Covariates:** other "variables" referred to as side information in
machine learning

**Standard error:** margin of error for a particular prediction with a
given confidence interval

```         
Example: if 62% of polled individuals support candidate A, the margin of error with 95% confidence must subtract the standard error for the sample size (e.g. 95% confidence of an ultimate population support of 58-66%)
```

**Predictions:**

Labeled High Risk but Didn't Re-Offend (FP) \* Black: 44.9% \* White:
23.5%

Labeled Low Risk but Did Re-Offend (FN) \* Black: 28% \* White: 47.7%

```         
Striving for equality across races in this table prevents equality of pairwise prediction across races
```

**Embedding:** avoid overfitting by using alternative variables as
proxies for variables like zip code \* Mean income of zip code \* Crime
rate in zip code \* Mean education level in zip code

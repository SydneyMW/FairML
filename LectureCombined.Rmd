---
title: "Lecture"
output: html_document
date: "2023-04-20"
editor_options: 
  markdown: 
    wrap: 72
---

# Lecture 1

### April 4, 2023

## Course Focus

-   Defining Fairness
-   Fairness Utility Tradeoff

## Course Resources

-   Read
    [Blog](https://github.com/ucdavis/FairMLCourse/blob/main/Blog.md)
    Daily
-   Syllabus
    [Procedures](https://github.com/matloff/nmGeneralCourseInfo/blob/master/RulesAndProcedures.md)
-   [Readings](https://github.com/ucdavis/FairMLCourse/blob/main/ReadingList.md)
    -   Read after covered by prof

**COMPAS Recidivism:** COMPAS algorithm was developed to provide
judges/prosecutors with predictions of the probability defendant will
commit another crime

-   A [Propublica
    article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    found immense racial bias in recidivism predictions

-   See [companion
    article](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)
    for technical details

**Simpson's Paradox:** Berkeley lawsuit for discrimination against women

-   Women tended to apply to more difficult/competitive programs

-   Overall acceptance rate for women was lower than that for men

**Why R?** R more frequently used in data science, aside from python in
deep learning

-   Fun fact: random forest developed by a woman

-   Random forest came from statistical community, more resources in R

------------------------------------------------------------------------

# Lecture 2

### April 6, 2023

## ProPublica Reading

Evaluating and remedying fairness/unfairness requires definition of
fairness

-   Only 20% of people predicted to commit violent crimes went on to do
    so (20% accuracy)

-   **P( commit crime \| predicted crime ) = 0.20**

Algorithm was particularly likely to falsely flag black defendants as
future criminals, labeling them at twice the rate as white defendants
(i.e. even less accurate for black defendants than for white defendants)

-   White defendants mislabeled as low risk more often than black
    defendants

-   Additional test isolating effect of race, age, and gender from
    criminal history and recidivism, still yielded 77% greater
    likelihood of black defendants being labeled as higher risk

**No P-Values:** skepticism of P-value relevance

After deal reached by prosecution and defense for plea bargain, judge
rejected in favor of heavier sentence on account of algorithm's high
risk prediction score

**Take education beyond 4th grade into account?**

-   Argument A: unfair to hold against individual

-   Argument B: lack of education yields higher risk for unemployment,
    which yields higher risk for crime

-   Argument C: agree to lighter sentence contingent upon participation
    in literacy program

**Taking out sensitive variables:** Removing ***race*** variable for
example, does not eliminate the effects of race. Other highly correlated
variables, such as ***education***, may serve as a ***proxy*** for race

-   Education

-   Past criminal record

-   Poverty

-   Employment

-   Social marginalization

**Risk score histograms:** show that algorithm generally assigns lower
risk to white defendants, but we still need to compare pairs of
comparable individuals of each category

-   Consider population/sample size

**Ceteris paribus:** "all else equal" Example: Find two defendants with
different race but same age/education/history/crime severity/etc, and
compare scores

**Covariates:** other "variables" referred to as side information in
machine learning

**Standard error:** margin of error for a particular prediction with a
given confidence interval

Example: if 62% of polled individuals support candidate A, the margin of
error with 95% confidence must subtract the standard error for the
sample size (e.g. 95% confidence of an ultimate population support of
58-66%)

**Predictions:**

Labeled High Risk but Didn't Re-Offend (FP)

-   Black: 44.9%

-   White: 23.5%

Labeled Low Risk but Did Re-Offend (FN)

-   Black: 28%

-   White: 47.7%

Striving for equality across races in this table prevents equality of
pairwise prediction across races

**Embedding:** avoid overfitting by using alternative variables as
proxies for variables like zip code

-   Mean income of zip code

-   Crime rate in zip code

-   Mean education level in zip code

------------------------------------------------------------------------

# Lecture 3

### April 6, 2023

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Simpson's Paradox

Two vars have positive relation:

-   (+) rel(x, y)

But the same vars have a negative relation when conditioned on 3rd var:

-   (-) rel(x, y \| z)

### Load UCB Data

```{r}
data("UCBAdmissions")
dim(UCBAdmissions)
dimnames(UCBAdmissions)
```

### Determine object structure:

**Structure:** This data is a 3D table with various levels per attribute

```{r}
str(UCBAdmissions)
```

**Margin table:** temporarily ignoring department attribute, find each
variable's "marginal counts" irrespective of department

```{r}
UCB.GA <- margin.table(UCBAdmissions, c(1,2))
UCB.GA
```

```{r}
univ <- apply(UCBAdmissions, c(1,2), sum)
univ
```

Note: on the surface women appear to be proportionally much more
frequently rejected than men

```{r}
prop.table(univ, 2)
```

Tool to develop: input data set, output graphically whether paradox
applies

Consider the number of unique values of x or y; if 2 then variable is
dichotomous. For dichotomous variables, perform alternative (Kendall's)
correlation measure; for non-dichotomous variables, use classical
correlation.

Graphical output:

-   Built-in base R graphics: plot(u,v,col=w) \# col is hue, barplot

-   Ggplot2 (ECS 132 appendix resource)

-   Lattice

Extra: find a data set & variables where paradox applies This data set
is discrete, but can be applied to continuous data

```{r}
library(vcd)
```

## Fourfold Chart

Invented by Edward Tufte

Shape:

-   Top row: admitted (L) and rejected (R) men

-   Bottom row: admitted (L) and rejected (R) women

Features:

-   In any 2 adjacent quadrants, the ratio of the areas shown is the oz
    ratio (Opposite of pi chart, constant angle with varying radius,
    whereas pi chart is varying angle with constant radius)

-   95% confidence interval around edge

    <div>

    -   $S.E. (\overline x ) = S / \sqrt(n)$

    -   $\overline x +/- 1.96*(S.E.(\overline x))$

    </div>

### Compare total acceptance

```{r}
UCB <- aperm(UCBAdmissions, c(2,1,3))
fourfold(margin.table(UCB, c(1,2)))
```

### Compare chart by department

Aside from top-left chart, departments have nearly identical quadrants
(similar M/F admission per dept)

Top-left chart: admissions favored women (admitted women at higher rates
than men)

```{r}
fourfold(UCBAdmissions, mfrow=c(2,3))
```

### Compare admission rates by department

```{r}
ftable(round(prop.table(UCBAdmissions, c(2,3)),2),
       row.vars="Dept", col.vars=c("Gender","Admit"))
```

Key terms to search:

-   Contingency tables

-   Log-linear model

-   X - explanatory variable(s), predictor(s), feature(s)

-   Y - response variable, outcome variable, target

Example:

-   X: weeks worked

-   Y: wage income

-   Z: age

-   Vertical axis will be correlations (find for z=1, z=2, ...)

-   Graph correlations: find correlations for each z, then graph

-   Bar heights are counts of correlations

-   Number of bins for z (age)?

```{r}
#UCB_G <- structable(Gender - Dept + Admit, data=UCBAdmissions)
#cotabplot(UCB_G)
```

------------------------------------------------------------------------

# Lecture 4

```{r}
library(qeML)
data(mlb)
head(mlb)
```

## Notation

**Y**: what we are predicting

**X**: what we are using to make predictions

**Indicator/Dichotomous Variables:** (In R) 2-level factors typically
1/0 Coded

-   X features can be indicators (dummy variables)

-   Y can be a single indicator or a set of indicator variables

**Arguments:**

-   Formal arg: \<y name\>

-   Actual arg: 'Weight'

-   Optional args: have default values or NA

### Example: KNN, Random Forest Train

-   Call form: `model fit <- qe<model name>(<data name>,<Y name>)`

-   Call:

```{r}
mlb1 <- mlb[,4:6]  # columns for height, weight and age
knnout <- qeKNN(mlb1,'Weight')  # fit k-Nearest Neighbor model
rfout <- qeRF(mlb1,'Weight')  # fit random forests model
```

### Example: Random Forest Predict

-   Call form: `predict(<model fit>, <new X value>)`

-   Call:

```{r}
predict(rfout,c(70,28))
```

### KNN Error (MAE)

```{r}
knnout$testAcc
```

### Random Forest Error (MAE)

```{r}
rfout$testAcc
```

**Default args in R:** Ideal for versatile user base

**Hyper parameters:** i.e. tuning parameters

-   Ex. number of nearest neighbors (e.g. k = 5 will use closest 5
    instances to predict new instance)

### Regression and Classification

**Regression Settings:** y is continuous or ordinal

**Classification Settings:** y is categorical

-   Y is converted to a set of indicator variables

-   $E(Y | X = t)$ reduces to $P(Y = 1 | X = t)$

-   Find probabilities for each category, then guess the one with the
    highest probability.

**The Regression Function:** $m(t) = E(Y | X = t)$

-   $m(t)$: the mean value of $Y$ in the subpopulation defined by
    $X = t$

-   Ex. $m(71,23)$ would be the mean weight among all players of height
    71 inches and 23 years old

-   Both regression and classification tasks make use of the regression
    function

------------------------------------------------------------------------

# Lecture 5

## Regression and Classification

**Regression Problem:** predicting continuous variables

-   Linear Regression is a special sub-case

**Classification Problem:** predicting categorical variables

**Regression Function:** unknown function we are estimating

-   Example: height (x) vs. mean weight (y) line

-   Example: height (x1) and age (x2) vs. mean weight (y) surface

-   Each ML method has its own tuning parameters, but all center around
    the regression function m(t)

**Proportions:** \# 1s / \# 0s = P(1)

-   P(1) \> 0.5 $\rightarrow$ predict yes (i.e. will pass exam)

-   P(1) \< 0.5 $\rightarrow$ predict no (i.e. will not pass exam)

## KNN

**Error**

-   For k = 10 with given holdout (i.e. 1000), run many tests with
    random holdout

-   Average error to measure performance for k = 10

**Distance Metrics**

-   Height could be ft, m, in; mutates scale

-   If using distance for height and age, where height is 55-75 and age
    is 20-30, we inadvertently place higher value on height than age

-   **Standardization:** generate $(X -\overline{X}) / \sigma$ for each
    point - essential for convergence in neural nets

-   **Scaler():** R function for standardization

**Tuning Hyperparameter k**

-   Too small: not enough data

-   Too large: insensitive

-   Default in qeKNN: k = 25

-   **Bias/Variance Tradeoff:** when k is small, variance of weight
    among sets of k people will vary significantly. When k is large,
    bias is introduced.

-   Can assign weights to variables as needed in qeKNN

```{r}
data(mlb)
mlb1 <- mlb[,4:6]  # columns for height, weight and age
head(mlb1)
```

Compare default k = 25, k = 10, and k = 3

Generate 50 random holdout sets and find mean error for a given k:

```{r}
replicMeans(50,"qeKNN(mlb1,'Weight')$testAcc")
#[1] 13.69573
#attr(,"stderr")
#[1] 0.1390298

replicMeans(50,"qeKNN(mlb1,'Weight',k=10)$testAcc")
#[1] 14.1965
#attr(,"stderr")
#[1] 0.1504877

replicMeans(50,"qeKNN(mlb1,'Weight',k=3)$testAcc")
#[1] 15.32858
#attr(,"stderr")
#[1] 0.1707612
```

## KNN Edge Bias

> "One potential problem is bias at the edge of a neighborhood. Say we
> are predict weight from height, and a new case involving a very tall
> person. Most data points in the neighborhood of this particular height
> value will be for people who are shorter than the new case. Those
> neighbors are thus likely to be lighter than the new case, making our
> predicted value for that case biased downward."

**qeKNN Solution:** set argument **smoothingFtn = loclin** to remove
linear trend within the neighborhood and improve edge case prediction
accuracy

**Example:** when predicting the weight using height and age, if a
person is particularly tall, the k-nearest neighbors will be mostly
shorter. Prediction will be **biased downward**.

------------------------------------------------------------------------

# Lecture 6

## Random Forest

**Benefits of many trees:**

-   Many orderings of variable

-   Smoothing effect on cutoffs; one instance that barely misses cutoff
    in one tree may be well within cutoff in another

-   Number of trees is a hyperparameter (goldilocks level): too few is
    like small k in KNN; too many is computationally expensive

**Choosing cutoffs:**

-   Hyperparameters

**Edge Bias:**

-   Similar to KNN

-   New case will experience bias down/up

-   qeRFgrf removes linear trend to deal with edge bias

**Boosting:**

-   Iteratively build up a sequence of trees, each of which is an
    improved update of the last

-   May start with nothing/root node/weak predictor

-   Tabular data: row/col arranged (csv, df, etc)

-   Grayscale image: matrix of values (32:32 px); not considered
    tabular; rather, sense data

-   xgboost

## Linear Model

"All models are wrong -- but some are useful"

**Assumption to Ignore:** Assume each sub-population possessing each
combination of variables/features is normally distributed

**Residual:** actual value - predicted value

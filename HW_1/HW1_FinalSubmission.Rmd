---
title: "HW1"
output: html_document
date: "2023-04-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

```{r}
library(qeML, quietly=T)
data(pef)
head(pef)
```

Get accuracy of predicting different variables:

```{r}
# Predict wage from all variables; MAE is > $25000
qeLin(pef,'wageinc')$testAcc

# Predict gender from alll variables; Incorrect 24% of the time
qeLogit(pef,'sex')$testAcc

# Predict occupation (6 jobs); Incorrect 64% of the time
qeLogit(pef,'occ')$testAcc
```

## Description:

Format: `takeALookAround(data,yName,sName,maxFeatureSetSize)`

`yName` and `sName` are the names of columns in the data frame `data`

The tool will investigate the impact of using various subsets of the feature set on

-   **(a)** prediction accuracy for Y of this feature set

-   **(b)** prediction accuracy for Y of this feature set plus S (how much are we giving up by NOT using S?)

-   **(c)** prediction accuracy for S of this feature set (the better the accuracy, the less fairness value there is in omitting S from prediction model)

The return value will be a data frame with 4 columns.

1.  A character string consisting of the names of the given feature set.
2.  The prediction accuracy for (a)
3.  The prediction accuracy for (b)
4.  The prediction accuracy for (c)

There will be one row in the d.f. for each feature set. All possible feature sets of size up to **maxFeatureSetSize** will be investigated.

### Case Study with 2 Features:

```{r}
data <- pef
yName <- "wageinc"
sName <- "sex"
maxFeatureSetSize <- 1000

# Select columns excluding yName and sName
featureSet <- colnames(data[,!names(data) %in% c(yName, sName)])
results <- data.frame(matrix(nrow=0, ncol=4))
colnames(results) <- c("features","Y","YS","Accuracy")

i <- 1
j <- 2

feature1 <- featureSet[i] # age
feature2 <- featureSet[j] # educ

data_a <- data[,c(feature1, feature2, yName)]
data_b <- data[,c(feature1, feature2, yName, sName)]
data_c <- data[,c(feature1, feature2, sName)]

if (is.numeric(data[1,yName])) {
  acc_a <- qeLin(data_a,yName)$testAcc
  acc_b <- qeLin(data_b,yName)$testAcc
} else if (is.factor(data[1,yName])) {
  acc_a <- qeLogit(data_b,yName)$testAcc
  acc_b <- qeLogit(data_b,yName)$testAcc
} else {
  acc_a <- NA
  acc_b <- NA
}

if (is.numeric(data[1,sName])) {
  acc_c <- qeLin(data_c,sName)$testAcc
} else if (is.factor(data[1,sName])) {
  acc_c <- qeLogit(data_c,sName)$testAcc
} else {
  acc_c <- NA
}

result <-c(paste(feature1,feature2,sep=","), acc_a, acc_b, acc_c)
results <- rbind.data.frame(results, result)
```

## Implementation:

```{r}
takeALookAround <- function(data, yName, sName, maxFeatureSetSize) {
  library(qeML)
  
  # Select columns excluding yName and sName
  featureSet <- colnames(data[,!names(data) %in% c(yName, sName)])
  results <- data.frame(matrix(nrow=0, ncol=4))
  
  # counter for feature set comparisons
  count <- 0
  
  # iterate through unique combinations of features
  for (i in 1:length(featureSet)) {
    for (j in i:length(featureSet)) {
      if (i != j) {
        feature1 <- featureSet[i] # i.e. age
        feature2 <- featureSet[j] # i.e. educ
        
        # a: predicting y from feature set
        data_a <- data[,c(feature1, feature2, yName)]
        # b: predicting y from feature set plus S
        data_b <- data[,c(feature1, feature2, yName, sName)]
        # c: predicting S from feature set
        data_c <- data[,c(feature1, feature2, sName)]
        
        # use qeLin on numeric data to predict y
        # use qeLogit on factor data to predict y
        if (is.numeric(data[1,yName])) {
          acc_a <- qeLin(data_a,yName)$testAcc # without S
          acc_b <- qeLin(data_b,yName)$testAcc # with S
        } else if (is.factor(data[1,yName])) {
          acc_a <- qeLogit(data_a,yName)$testAcc # without S
          acc_b <- qeLogit(data_b,yName)$testAcc # with S
        } else {
          acc_a <- NA # invalid data
          acc_b <- NA # invalid data
        }
        # use qeLin on numeric data to predict S
        # use qeLogit on factor data to predict S
        if (is.numeric(data[1,sName])) {
          acc_c <- qeLin(data_c,sName)$testAcc # predict S
        } else if (is.factor(data[1,sName])) {
          acc_c <- qeLogit(data_c,sName)$testAcc # predict S
        } else {
          acc_c <- NA
        }
        # names of features, accuracy a, b, and c
        result <-c(paste(feature1,feature2,sep=","), acc_a, acc_b, acc_c)
        results <- rbind.data.frame(results, result)
        count <- count+1
        # break if limit met
        if (count == maxFeatureSetSize) break
      }
    }
    # break if limit met
    if (count == maxFeatureSetSize) break
  }
  print(count)
  colnames(results) <- c("features","Y_acc","YS_acc","S_acc")
  # df with names and accuracies
  return(results)
}
```

## Test Function on Example

```{r}
wageinc_sex <- takeALookAround(data=pef, 
                               yName="wageinc", 
                               sName="sex", 
                               maxFeatureSetSize=100)
print(wageinc_sex)
```

```{r}
wageinc_educ <- takeALookAround(data=pef, 
                               yName="wageinc", 
                               sName="educ", 
                               maxFeatureSetSize=100)
print(wageinc_educ)
```

# Problem 2

## Description:

### Simpson's Paradox

relation(X,Y) is + but relation(X,Y\|Z) is -, or vice versa

Usually, e.g. in the **UCBAdmissions** data, the variables X, Y and Z are all categorical. Your code here will explore whether SP seems to hold for numeric X and Y (and maybe Z) for some given dataset

### Variables

-   **data** is an R data frame or equivalent

-   **xName** is the name of the X column

-   **yName** is the name of the Y column

-   **zName** is the name of the Z column

-   **numZvals** is the number of intervals to break Z into in the case where Z is a continuous variable

X and Y should either be continuous numeric variables or dichotomous R factors.

```{r}
library(qeML)
library(Kendall)
data(pef)
head(pef)
```

We will define "relation" as one of two types of correlation:

-   If both X and Y are continuous, use the classical Pearson correlation, available in R as **cor()**.

-   If at least one of X and Y is dichotomous, use *Kendall's tau* correlation, code available in the **Kendall** package. You should be able to use **install.packages()** to get this code.

```{r}
cor(pef[,"age"],pef[,"wageinc"])
```

## Implementation

```{r}
numericSimpsonfunction <- function(data,xName,yName,zName,numZvals=NULL){
  #sort by Z value
  df_ordered <- data[order(data[,zName]),]
  size <- length(df_ordered[,xName])
  
  if (is.numeric(data[, zName])){
    interval_val <- ceiling(df_ordered[size, zName]/numZvals)
    i_val <- interval_val
  } else {
    #Separate Z into levels if it is categorical
    interval_val <- levels(data[, zName])
    numZvals = length(interval_val)
    #Initialize i_val to avoid processing error, however this assignment will not be 
    #i_val contains the next item to search for so that the values in interval_val remain unchanged
    i_val <- interval_val[1]
  }
  #p_points is an array containing the index of the last occurrence of an element in the data
  p_points <- c(1)
  #Get a vector showing the indices of intervals 
  for (i in 1:numZvals){
    val <- match(i_val, df_ordered[,zName])
    #If no match exists, find the closest element
    if (is.na(val)){
      val <- which.min(abs(df_ordered[,zName] - i_val))
    }
    if (is.numeric(data[,zName])){
      #Include the upper bound and exit the loops at the last iteration
      if (i == numZvals){
        p_points <- append(p_points, size)
        break
      }
      #otherwise continue through
      p_points <- append(p_points, val)
      i_val <- i_val + interval_val
    } else{
      #First element in p_points should be 1, not the the last occurrence of the first category
      #Therefore, set i_val to the next element to find and continue the loop
      if (i == 1){
        i_val <- interval_val[i+1]
        next
        }
      p_points <- append(p_points, val) 
      #increment i_val
      i_val <- interval_val[i+1]
      #Include the upperbound
      if (i == numZvals){
        p_points <- append(p_points, size)
      }
    }
  }

  #CorrList will contain an array of all the calculated correlations
  CorrList <- c()
  for (i in 1:numZvals){
    #p_points[i] is lower bound of each partition, p_points[i+1] is the upper bound
    xVar <- as.numeric(df_ordered[p_points[i]:p_points[i+1], xName])
    yVar <- as.numeric(df_ordered[p_points[i]:p_points[i+1], yName])
    
    if (is.factor(data[1,xName]) || is.factor(data[1,yName])){
      Corr <- cor(xVar, yVar, method = 'kendall', use = 'complete.obs')
    }
    else if (is.numeric(data[1,xName]) && is.numeric(data[1, yName])){
      Corr <- cor(xVar, yVar, use = 'complete.obs')
    }
    CorrList <- append(CorrList, Corr)
  }
  
  #=====================Graphing==========================
  header <- paste("X = ", xName, ", Y = ", yName, ", Z = ", zName, sep = "")
  lb_list <- c()
  for (i in 1:numZvals){
    #if its numeric none of the bounds information
    if (is.numeric(data[,zName])){
      if (i == 1){
        #first and second give the numeric values of the Z-Bounds
        first <- df_ordered[p_points[i]+1, zName]
        second <- df_ordered[p_points[i+1], zName]
      }else{
        first <- df_ordered[p_points[i], zName]
        second <- df_ordered[p_points[i+1], zName]
      }
      label <- paste("(", first, ", ", second, "]", sep = "")
    }else{
      if (i == 1){
        label <- df_ordered[p_points[i]+1, zName]
      }else{
        label <- df_ordered[p_points[i], zName]
      }
    }
    lb_list <- append(lb_list, label)
  }
  uncCor <- cor(as.numeric(data[,xName]), as.numeric(data[,yName]))
  sub <- paste("Unconditional Correlation = ", uncCor, sep = "")
  barplot(CorrList, 
          col = palette(),
          names.arg = lb_list,
          main = header,
          ylab = "cors",
          xlab = "zLvls",
          ylim = c(-1,1),
          sub = sub,
          col.sub = "blue"
        )
  
  abline(h=0, col = "black")
  abline(h = uncCor, col = "blue")
}
```

## Test Samples

```{r}
numericSimpsonfunction(pef, 'sex', 'wageinc','wkswrkd',4)
```

```{r}
data <- pef
xName <- "age" # continuous numeric
yName <- "wkswrkd" # continuous numeric
zName <- "wageinc" # numeric
numZvals <- 5
numericSimpsonfunction(data,xName,yName,zName,numZvals)
```

```{r}
data <- pef
xName <- "age" # continuous numeric
yName <- "sex" # dichotomous factor
zName <- "wageinc" # numeric
numericSimpsonfunction(data,xName,yName,zName,2)
```

```{r}
data <- pef
xName <- "age" # continuous numeric
yName <- "wageinc" # continuous numeric
zName <- "sex" # categorical
numericSimpsonfunction(data,xName,yName,zName,numZvals=NULL)
```

```{r}
numericSimpsonfunction(pef,'age','wageinc','wkswrkd',4)
```

```{r}
numericSimpsonfunction(pef,'sex','wageinc','wkswrkd',5)
```

```{r}
numericSimpsonfunction(pef,'age','wageinc','occ',4)
```

# Extra Credit B

It would be nice to condition on two factors, e.g. gender and occupation. There Z would be an R factor with 2 X 6 = 12 levels, representing a categorical variable of 12 categories.

Write a function with call form

    superFactor(f1,f2)

which returns a new factor that is a combination of factors **f1** and **f2** as described above.

```{r}
superFactor <- function(f1,f2) {
  f3 <- as.factor(paste(f1,f2,sep="_"))
  return (f3)
}
```

```{r}
unique(superFactor(pef$sex, pef$occ))
```

```{r}
unique(superFactor(pef$sex, pef$educ))
```

# Extra Credit A

Find a real dataset, in which at least one of X and Y is a continuous variable, in which SP hold. Write your jode as a function call **spExample()** (no arguments). It will fetch the dataset, perform any needed preprocessing, and then call **numericSimpson()**.

```{r}
#install.packages(c("devtools"))
#devtools::install_github("ldurazo/kaggler")
library(readr)
library(kaggler)
kgl_auth(creds_file = 'kaggle.json')
response <- kgl_datasets_download_all(owner_dataset = "wduckett/californiaddsexpenditures")

download.file(response[["url"]], "temp.zip", mode="wb")
unzip_result <- unzip("temp.zip", overwrite = TRUE)
dds <- read_csv("californiaDDSDataV2.csv")
dds <- data.frame(dds)
```

## Ethnicity-Expenditure Correlation Reduces when Age-Stratified

```{r}
dds$Ethnicity <- as.factor(dds$Ethnicity)
dds$GenderBin <- as.numeric(dds$Gender=="Female")
dds$Age.Cohort <- as.factor(dds$Age.Cohort)

#numericSimpson(dds, "Expenditures", "GenderBin","Ethnicity")

data <- dds
xName <- "Expenditures" # continuous numeric
yName <- "Ethnicity" # continuous numeric
zName <- "Age.Cohort" # categorical
numericSimpsonfunction(data,xName,yName,zName,numZvals=NULL)
```
